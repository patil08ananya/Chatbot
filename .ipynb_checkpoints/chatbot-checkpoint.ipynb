{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a27bcd5d-c143-435e-84a2-20885129d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nltk\n",
    "import string \n",
    "import random\n",
    "import os\n",
    "import pandas as pd \n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595734c-8373-45e9-ae4c-97398d52c3d0",
   "metadata": {},
   "source": [
    "### Reading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3eaa9aee-d788-4a7d-b71f-8f42375c5ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anany\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ac016be6-f8e1-4a57-83bc-474339d8de1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anany\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  #download module for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "878f6f66-6804-455e-97a6-ed5a7a73f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = open('Python.txt', 'r',encoding='utf-8')       #f = data1, #m = data2\n",
    "data2 = open('modules.txt','r',encoding = 'utf-8')\n",
    "checkpoint = \"./chatbot_weights.ckpt\"\n",
    "raw_data1 = data1.read().lower()\n",
    "raw_data2 = data2.read().lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed29b55-c628-4105-b22f-a8f093a22715",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b555fa47-35f1-40ab-b431-9b11b970323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "sent_tokens1 = nltk.sent_tokenize(raw_data1)\n",
    "word_tokens1 = nltk.word_tokenize(raw_data1)\n",
    "sent_tokens2 = nltk.sent_tokenize(raw_data2)\n",
    "word_tokens2 = nltk.word_tokenize(raw_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b11ecc6-694d-44f7-a24e-906eee9e4a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmer = WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punc_dict = dict((ord(punct),None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64660bc-01f7-482f-b6ee-4c0065521fa5",
   "metadata": {},
   "source": [
    "Basic intro and QnA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "17a34f84-069f-4556-a72d-649c1a23b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "Intro = [\"My name is PyMentor.\",\"I am PyMentor :) \",\"My name is PyMentor and I am happy to solve your queries :) \",\"Hi I am PyMentor, Ask me anything related to Python\",\"Hi I am PyMentor, How can help you ?\"]\n",
    "greeting = ['hello',\"hey\",\"hi\",\"hie\",\"hellu\",\"hiiii\",\"hello there\",\"hey there\",\"hi there\",\"wssup\"]\n",
    "greeting_response = [\"hi\",\"hello\",\"hi there\"]\n",
    "Basic_Q1 = (\"what is python ?\",\"what is python\",\"what is python?\",\"what is python.\")\n",
    "Basic_Ans1 = \"Python is an interpreted, object-oriented, high-level programming language with dynamic semantics. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured, object-oriented and functional programming.\"\n",
    "Basic_Q2 = (\"what is module\",\"what is module.\",\"what is module \",\"what is module ?\",\"what is module?\",\"what is module in python\",\"what is module in python.\",\"what is module in python?\",\"what is module in python ?\")\n",
    "Basic_Ans2 = [\"Modules in Python are files containing Python code, which can include variables, functions, and classes. They help organize code into reusable parts, making it easier to manage and maintain large projects. You can import modules into your Python programs to access their functionality.\",\"Consider a module to be the same as a code library.\",\"A file containing a set of functions you want to include in your application.\",\"A module can define functions, classes and variables. A module can also include runnable code. Grouping related code into a module makes the code easier to understand and use.\"]\n",
    "goodbye = [\"Welcome!\",\"Happy to help\",\"My pleasure\",\"I am glad I could help you!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5322bac-2265-4059-a4f0-cedc600e9162",
   "metadata": {},
   "source": [
    "helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "721fd123-1dfe-4ece-a1ae-a4fcfe54cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in greeting:\n",
    "            return random.choice(greeting_response)\n",
    "\n",
    "def basic1(sentence):\n",
    "    for word in Basic_Q1:\n",
    "        if sentence.lower() == word:\n",
    "            return Basic_Ans1\n",
    "def basic2(sentence):\n",
    "    for word in Basic_Q2:\n",
    "        if sentence.lower() == word:\n",
    "            return random.choice(Basic_Ans2)\n",
    "\n",
    "def Introduction(sentence):\n",
    "    return random.choice(Intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "183ea183-0c5f-4f72-8926-884f06b24e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea5448d-9998-45ec-99c2-718cc42d4e11",
   "metadata": {},
   "source": [
    "response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "472ef4a6-5eff-415e-a851-bcd94c446073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reponse generation \n",
    "def response1(user_response):\n",
    "    bot_response = ''\n",
    "    sent_tokens1.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer = LemNormalize, stop_words = 'english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens1)\n",
    "    vals = cosine_similarity(tfidf[-1],tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf == 0):\n",
    "        bot_response = bot_response + \"I'm sorry, I do not understand!\"\n",
    "        return bot_response\n",
    "    else:\n",
    "        bot_response = bot_response + sent_tokens1[idx]\n",
    "        return bot_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b20609c0-cd8a-4505-ad71-b72a2870ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response2(user_response):\n",
    "    bot_response = ''\n",
    "    sent_tokens2.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer = LemNormalize, stop_words = 'english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens2)\n",
    "    vals = cosine_similarity(tfidf[-1],tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf == 0):\n",
    "        bot_response = bot_response + \"I'm sorry, I do not understand!\"\n",
    "        return bot_response\n",
    "    else:\n",
    "        bot_response = bot_response + sent_tokens2[idx]\n",
    "        return bot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6fd586a7-9568-4b4f-8130-89f37ef8966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chat(user_response,flag):\n",
    "    user_response = user_response.lower()\n",
    "    keyword = \"module \"\n",
    "    keywordone = \" module\" \n",
    "    keywordsec = \" module \"\n",
    "\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response == 'thanks' or user_response == 'thank you'):\n",
    "            flag = False\n",
    "            print(\"PyMentor: \", random.choice(goodbye))\n",
    "            #return random.choice(\"Welcome!\",\"Happy to help\",\"My pleasure\",\"I am glad I could help you!\")\n",
    "            return flag\n",
    "        elif(basic2(user_response)!=None):\n",
    "            print(\"PyMentor: \",basic2(user_response))\n",
    "            #return basic2(user_response)\n",
    "            return flag\n",
    "        else:\n",
    "            if(user_response.find(keyword) != -1 or user_response.find(keywordone) != -1 or user_response.find(keywordsec)!=-1):\n",
    "                print(\"PyMentor: \",response2(user_response))\n",
    "                #return response2(user_response)\n",
    "                sent_tokens2.remove(user_response)\n",
    "                return flag\n",
    "            elif(greet(user_response)!= None):\n",
    "                print(\"PyMentor: \",greet(user_response))\n",
    "                #return greet(user_response)\n",
    "                return flag\n",
    "            elif(user_response.find(\"your name\") != -1 or user_response.find(\" your name\") != -1 or user_response.find(\"your name \") != -1 or user_response.find(\" your name \") != -1): \n",
    "                print(\"PyMentor: \",Intro(user_response))\n",
    "                #return Intro(user_response)\n",
    "                return flag\n",
    "            elif(basic1(user_response)!= None): \n",
    "                print(\"PyMentor: \",basic1(user_response))\n",
    "                #return basic1(user_response)\n",
    "                return flag\n",
    "            else:\n",
    "                print(\"PyMentor: \",response1(user_response))\n",
    "                #return response1(user_response)\n",
    "                sent_tokens1.remove(user_response)\n",
    "                return flag\n",
    "    else:\n",
    "        flag = False\n",
    "        print(\"PyMentor: Bye\")\n",
    "        #return \"Bye! take care!\"\n",
    "        return flag\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5a886433-084d-4c37-8647-7ee6c403b55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " thanks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyMentor:  My pleasure\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "while(flag == True):\n",
    "    print(\"User: \", end = '')\n",
    "    user_response = input()\n",
    "    flag = chat(user_response,flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701fcf26-96bf-419d-8e5e-807a3f9c1134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d090f5c-1481-434c-9903-65b8a793bdbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
